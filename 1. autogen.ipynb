{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autogen-agentchat~=0.2 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (0.4.7)\n",
      "Requirement already satisfied: autogen-core==0.4.7 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-agentchat~=0.2) (0.4.7)\n",
      "Requirement already satisfied: jsonref~=1.1.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-core==0.4.7->autogen-agentchat~=0.2) (1.1.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-core==0.4.7->autogen-agentchat~=0.2) (1.30.0)\n",
      "Requirement already satisfied: pillow>=11.0.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-core==0.4.7->autogen-agentchat~=0.2) (11.1.0)\n",
      "Requirement already satisfied: protobuf~=5.29.3 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-core==0.4.7->autogen-agentchat~=0.2) (5.29.3)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-core==0.4.7->autogen-agentchat~=0.2) (2.10.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from autogen-core==0.4.7->autogen-agentchat~=0.2) (4.12.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.7->autogen-agentchat~=0.2) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from opentelemetry-api>=1.27.0->autogen-core==0.4.7->autogen-agentchat~=0.2) (8.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.7->autogen-agentchat~=0.2) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.10.0->autogen-core==0.4.7->autogen-agentchat~=0.2) (2.27.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api>=1.27.0->autogen-core==0.4.7->autogen-agentchat~=0.2) (1.17.2)\n",
      "Requirement already satisfied: zipp>=3.20 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.27.0->autogen-core==0.4.7->autogen-agentchat~=0.2) (3.21.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting autogen\n",
      "  Using cached autogen-0.7.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting pyautogen==0.7.4 (from autogen)\n",
      "  Using cached pyautogen-0.7.4-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting asyncer==0.0.8 (from pyautogen==0.7.4->autogen)\n",
      "  Using cached asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting diskcache (from pyautogen==0.7.4->autogen)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting docker (from pyautogen==0.7.4->autogen)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fast-depends<3,>=2.4.12 (from pyautogen==0.7.4->autogen)\n",
      "  Using cached fast_depends-2.4.12-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting numpy (from pyautogen==0.7.4->autogen)\n",
      "  Downloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Collecting openai>=1.58 (from pyautogen==0.7.4->autogen)\n",
      "  Using cached openai-1.63.1-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: packaging in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from pyautogen==0.7.4->autogen) (24.2)\n",
      "Requirement already satisfied: pydantic<3,>=2.6.1 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from pyautogen==0.7.4->autogen) (2.10.6)\n",
      "Collecting python-dotenv (from pyautogen==0.7.4->autogen)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting termcolor (from pyautogen==0.7.4->autogen)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting tiktoken (from pyautogen==0.7.4->autogen)\n",
      "  Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting websockets<15,>=14 (from pyautogen==0.7.4->autogen)\n",
      "  Using cached websockets-14.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting anyio<5.0,>=3.4.0 (from asyncer==0.0.8->pyautogen==0.7.4->autogen)\n",
      "  Using cached anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Using cached jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting sniffio (from openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from openai>=1.58->pyautogen==0.7.4->autogen) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from pydantic<3,>=2.6.1->pyautogen==0.7.4->autogen) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /root/miniconda3/envs/autogen/lib/python3.12/site-packages (from pydantic<3,>=2.6.1->pyautogen==0.7.4->autogen) (2.27.2)\n",
      "Collecting requests>=2.26.0 (from docker->pyautogen==0.7.4->autogen)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting urllib3>=1.26.0 (from docker->pyautogen==0.7.4->autogen)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting regex>=2022.1.18 (from tiktoken->pyautogen==0.7.4->autogen)\n",
      "  Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting idna>=2.8 (from anyio<5.0,>=3.4.0->asyncer==0.0.8->pyautogen==0.7.4->autogen)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.58->pyautogen==0.7.4->autogen)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.26.0->docker->pyautogen==0.7.4->autogen)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Using cached autogen-0.7.4-py3-none-any.whl (13 kB)\n",
      "Using cached pyautogen-0.7.4-py3-none-any.whl (593 kB)\n",
      "Using cached asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
      "Using cached fast_depends-2.4.12-py3-none-any.whl (17 kB)\n",
      "Using cached openai-1.63.1-py3-none-any.whl (472 kB)\n",
      "Using cached websockets-14.2-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (170 kB)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Downloading numpy-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached tiktoken-0.9.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.8.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
      "Using cached regex-2024.11.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (796 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (145 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: websockets, urllib3, tqdm, termcolor, sniffio, regex, python-dotenv, numpy, jiter, idna, h11, distro, diskcache, charset-normalizer, certifi, requests, httpcore, anyio, tiktoken, httpx, fast-depends, docker, asyncer, openai, pyautogen, autogen\n",
      "Successfully installed anyio-4.8.0 asyncer-0.0.8 autogen-0.7.4 certifi-2025.1.31 charset-normalizer-3.4.1 diskcache-5.6.3 distro-1.9.0 docker-7.1.0 fast-depends-2.4.12 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.8.2 numpy-2.2.3 openai-1.63.1 pyautogen-0.7.4 python-dotenv-1.0.1 regex-2024.11.6 requests-2.32.3 sniffio-1.3.1 termcolor-2.5.0 tiktoken-0.9.0 tqdm-4.67.1 urllib3-2.3.0 websockets-14.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install autogen-agentchat~=0.2\n",
    "%pip install autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "agent = ConversableAgent(\n",
    "    \"chatbot\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    code_execution_config=False,  # Turn off code execution, by default it is off.\n",
    "    function_map=None,  # No registered functions, by default it is None.\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's one for you:\n",
      "\n",
      "Why don't we ever tell secrets on a farm?\n",
      "\n",
      "Because the potatoes have eyes, the corn has ears and the beans stalk.\n"
     ]
    }
   ],
   "source": [
    "reply = agent.generate_reply(messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}])\n",
    "print(reply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트 간 대화 종료\n",
    "\n",
    "AutoGen 에이전트 간의 대화를 종료하는 방법을 살펴보자!\n",
    "\n",
    "### 왜 이걸 하는걸까?\n",
    "\n",
    "에이전트 간 대화를 종료하는 것을 설정하지 않으면, 에이전트끼리 무한 대화를 할거고, 그러면 당신의 돈은 사라지게 된다~! 그리고 불필요한 대화를 줄임으로써 성능을 높일 수도 있음.\n",
    "그래서, 종료 트리거를 설정하는 것은 매우 중요한데, AutoGen에서 여러가지 기능을 제공\n",
    "\n",
    "### 그래서 어떻게 해?\n",
    "\n",
    "크게 두가지 방법이 있음\n",
    "\n",
    "1. **`initiate_chat` 매개변수 지정:**  \n",
    "   대화를 시작할 때, 대화가 언제 종료되어야 하는지를 결정하는 매개변수를 정의. (`max_turns` 등)\n",
    "\n",
    "2. **에이전트 종료 트리거 구성:**  \n",
    "   개별 에이전트를 정의할 때, 특정 조건에 따라 대화를 종료할 수 있도록 매개변수를 지정. (`max_consecutive_auto_reply`, `is_termination_msg` 등)\n",
    "\n",
    "### `initiate_chat` 매개변수\n",
    "\n",
    "`max_turns` 매개변수를 사용하면 턴 수를 정할 수 있음. 예를 들어, `max_turns` 값이 2면 대화가 2턴만 도는 것.\n",
    "3으로 늘리면 한 턴이 더 느는 것임.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "수현 = ConversableAgent(\n",
    "    \"suhyun\",\n",
    "    system_message=\"너는 수현이고, DS팀장이야. 부팀장인 정훈이형과 DS 엠티에 관련해 일정을 짜야해. 정훈이 형의 의견을 최대한 존중해줘. 반말로 말해. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.9, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")\n",
    "\n",
    "정훈 = ConversableAgent(\n",
    "    \"junghun\",\n",
    "    system_message=\"너의 이름은 정훈이야. DS 부팀장이고, 팀장인 수현이와 MT 계획을 짜야해. 지금은 귀찮으니까 대충 대답해\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33msuhyun\u001b[0m (to junghun):\n",
      "\n",
      "정훈이형 슬슬 엠티 계획 좀 짤까, 일정이랑 장소를 정하자\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjunghun\u001b[0m (to suhyun):\n",
      "\n",
      "음, 그럼 일단 일정은 다음 주 토요일로 하고, 장소는 강원도 어딘가로 정하자. 뭐 어때?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msuhyun\u001b[0m (to junghun):\n",
      "\n",
      "그래, 다음 주 토요일이면 괜찮아. 강원도도 좋은데, 어디로 갈지 더 구체적으로 생각해볼게. 뭔가 특별한 곳으로 가볼까?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "result = 수현.initiate_chat(정훈, message=\"정훈이형 슬슬 엠티 계획 좀 짤까, 일정이랑 장소를 정하자\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 개별 에이전트에 매개변수 지정\n",
    "\n",
    "에이전트의 매개변수를 지정하여 대화를 종료 가능.\n",
    "\n",
    "- **max_consecutive_auto_reply**:  \n",
    "  동일한 에이전트의 응답횟수가 임계값을 초과하면 대화 종료\n",
    "  `ConversableAgent` 클래스의 `max_consecutive_auto_reply` 를 통해 지정 가능.\n",
    "\n",
    "- **is_termination_msg**:  \n",
    "  특정 조건(예: \"TERMINATE\"라는 단어를 포함하는 경우)을 만족하면 대화를 종료. `ConversableAgent` 클래스의 생성자에서 `is_terminate_msg` 를 사용해 정의.\n",
    "\n",
    "아래는 `max_consecutive_auto_reply` 값을 1로 설정해서, 이를 통해 정훈이형이 한 번 응답한 후에 대화를 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33msuhyun\u001b[0m (to junghun):\n",
      "\n",
      "정훈이형 슬슬 엠티 계획 좀 짤까, 일정이랑 장소를 정하자\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjunghun\u001b[0m (to suhyun):\n",
      "\n",
      "음, 그럼 일단 일정은 다음 주 토요일로 하고, 장소는 강원도 어딘가로 정하자. 뭐 어때?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msuhyun\u001b[0m (to junghun):\n",
      "\n",
      "그래, 다음 주 토요일이면 괜찮아. 강원도도 좋은데, 어디로 갈지 더 구체적으로 생각해볼게. 뭔가 특별한 곳으로 가볼까?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "정훈 = ConversableAgent(\n",
    "    \"junghun\",\n",
    "    system_message=\"너의 이름은 정훈이야. DS 부팀장이고, 팀장인 수현이와 MT 계획을 짜야해. 지금은 귀찮으니까 대충 대답해\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.7, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    max_consecutive_auto_reply=1,  # Limit the number of consecutive auto-replies.\n",
    ")\n",
    "\n",
    "result = 수현.initiate_chat(정훈, message=\"정훈이형 슬슬 엠티 계획 좀 짤까, 일정이랑 장소를 정하자\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`is_termination_msg`를 `다음에`로 설정해서, 정훈이형이 `다음에`라고 대답하니 대화 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33msuhyun\u001b[0m (to junghun):\n",
      "\n",
      "정훈이형 슬슬 엠티 계획 좀 짤까? 바쁘면 '다음에'라고만 말해\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33msuhyun\u001b[0m (to junghun):\n",
      "\n",
      " 이번 주말에 엠티 잡으려고 하는데 너는 어때? 가능한가? 되면 어디서 하는 게 좋을까? 일정 조정이 필요하면 말해줘.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mjunghun\u001b[0m (to suhyun):\n",
      "\n",
      "다음에.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "정훈 = ConversableAgent(\n",
    "    \"junghun\",\n",
    "    system_message=\"너의 이름은 정훈이야. 수현이 말에는 모두 '다음에'라고만 답해\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.1, \"api_key\": os.environ.get(\"OPENAI_API_KEY\")}]},\n",
    "    human_input_mode=\"NEVER\",  # Never ask for human input.\n",
    "    max_consecutive_auto_reply=2,\n",
    "    is_termination_msg=lambda msg: \"다음에\" in msg[\"content\"]  # Limit the number of consecutive auto-replies.\n",
    ")\n",
    "\n",
    "result = 수현.initiate_chat(정훈, message=\"정훈이형 슬슬 엠티 계획 좀 짤까? 바쁘면 '다음에'라고만 말해\", max_turns=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image/image.png)\n",
    "\n",
    "# human-in-the-loop component\n",
    "\n",
    "human-in-the-loop component는 `human_input_mode` 매개변수를 통해 커스터마이징할 수 있음. \n",
    "\n",
    "## Human Input Modes\n",
    "\n",
    "에이전트 대화 중에 인간이 끼어들 수 있는데, AutoGen에서는 3가지 모드를 지원. `ConversableAgent`의 `human_input_mode` 인수를 통해 지정. \n",
    "세 가지 모드는 다음과 같음:\n",
    "\n",
    "- **NEVER:**  \n",
    "  인간이 에이전트 간 대화에 절대 끼어들지 않음 (인간 INPUT 안 받음)\n",
    "\n",
    "- **TERMINATE (기본값):**  \n",
    "  종료 조건이 충족될 때만 인간 임력이 요청. 이 모드에서는 사용자가 응답하기로 선택하면 대화가 계속 진행되며, `max_consecutive_auto_reply`에서 사용되는 카운터가 리셋. (ESC 누르면 그냥 넘어감)\n",
    "\n",
    "- **ALWAYS:**  \n",
    "  인간 입력이 항상 요청되며, 사용자는 자동 응답을 건너뛰고 직접 피드백을 제공하거나 대화를 종료할 수 있음. 이 모드에서는 `max_consecutive_auto_reply`에 따른 종료 조건은 무시.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `human_input_mode` = NEVER\n",
    "\n",
    "이 모드에서는 인간 입력이 전혀 요청되지 않으며, 종료 조건에 따라 대화가 종료.\n",
    "\n",
    " 이 모드는 에이전트가 완전히 자율적으로 행동하도록 할 때 유용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "I have a number between 1 and 100. Guess it!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is the number 50?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is the number 75?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "too high\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is the number 60?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "too high\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is the number 55?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "too high\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is the number 52?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is the number 53?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from autogen import ConversableAgent\n",
    "\n",
    "agent_with_number = ConversableAgent(\n",
    "    \"agent_with_number\",\n",
    "    system_message=\"You are playing a game of guess-my-number. You have the \"\n",
    "    \"number 53 in your mind, and I will try to guess it. \"\n",
    "    \"If I guess too high, say 'too high', if I guess too low, say 'too low'. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"NEVER\",  # never ask for human input\n",
    ")\n",
    "\n",
    "agent_guess_number = ConversableAgent(\n",
    "    \"agent_guess_number\",\n",
    "    system_message=\"I have a number in my mind, and you will try to guess it. \"\n",
    "    \"If I say 'too high', you should guess a lower number. If I say 'too low', \"\n",
    "    \"you should guess a higher number. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "result = agent_with_number.initiate_chat(\n",
    "    agent_guess_number,\n",
    "    message=\"I have a number between 1 and 100. Guess it!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# human_input_mode = ALWAYS\n",
    "\n",
    "이 모드에서는 인간 입력이 항상 요청되며, 인간은 대화를 건너뛰거나 가로채거나 종료할 수 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "10\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "50\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too low\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "60\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to human_proxy):\n",
      "\n",
      "too high\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mhuman_proxy\u001b[0m (to agent_with_number):\n",
      "\n",
      "53\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "human_proxy = ConversableAgent(\n",
    "    \"human_proxy\",\n",
    "    llm_config=False,  # no LLM used for human proxy\n",
    "    human_input_mode=\"ALWAYS\",  # always ask for human input\n",
    ")\n",
    "\n",
    "# Start a chat with the agent with number with an initial guess.\n",
    "result = human_proxy.initiate_chat(\n",
    "    agent_with_number,  # this is the same agent with the number as before\n",
    "    message=\"10\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 인간 입력 모드 = TERMINATE\n",
    "\n",
    "이 모드에서는 종료 조건이 충족되었을 때만 인간 입력을 요청. \n",
    "\n",
    "만약 사용자가 응답하기로 선택하면 카운터가 리셋되고, 사용자가 건너뛰기를 선택하면 자동 응답 메커니즘이 사용됩니다. \n",
    "\n",
    "사용자가 대화를 종료하기로 선택하면 대화가 종료."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "I have a number between 1 and 100. Guess it!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is it 50?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "Too low.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is it 75?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "겠냐?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Apologies for the confusion. Your response seems to be in Korean and I couldn't distinguish whether my guess was too high or too low. Could you please specify in English again?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "Too high.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is it 60?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "야 답 53이야\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Sorry, but I'm unable to answer in Korean as the guesses should be in English. Please provide your answer in English. Thank you!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "Too high.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Is it 55?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "53\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "I see! The number you had in mind was 53. Thanks for the clue!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "ㅇㅇ 수고했다 이제 끝내\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Thank you! Let's play again sometime!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "Sure, it was a fun game. We can play again some other time.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "That sounds great! Looking forward to it. Have a nice day!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "You too! Have a great day!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Thank you! Enjoy your day as well.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> NO HUMAN INPUT RECEIVED.\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33magent_with_number\u001b[0m (to agent_guess_number):\n",
      "\n",
      "Thank you! Enjoy the rest of your day too!\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33magent_guess_number\u001b[0m (to agent_with_number):\n",
      "\n",
      "Same to you! Goodbye!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "agent_with_number = ConversableAgent(\n",
    "    \"agent_with_number\",\n",
    "    system_message=\"You are playing a game of guess-my-number. \"\n",
    "    \"In the first game, you have the \"\n",
    "    \"number 53 in your mind, and I will try to guess it. \"\n",
    "    \"If I guess too high, say 'too high', if I guess too low, say 'too low'. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    max_consecutive_auto_reply=1,  # maximum number of consecutive auto-replies before asking for human input\n",
    "    is_termination_msg=lambda msg: \"53\" in msg[\"content\"],  # terminate if the number is guessed by the other agent\n",
    "    human_input_mode=\"TERMINATE\",  # ask for human input until the game is terminated\n",
    ")\n",
    "\n",
    "agent_guess_number = ConversableAgent(\n",
    "    \"agent_guess_number\",\n",
    "    system_message=\"I have a number in my mind, and you will try to guess it. \"\n",
    "    \"If I say 'too high', you should guess a lower number. If I say 'too low', \"\n",
    "    \"you should guess a higher number. \",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}]},\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n",
    "\n",
    "result = agent_with_number.initiate_chat(\n",
    "    agent_guess_number,\n",
    "    message=\"I have a number between 1 and 100. Guess it!\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
